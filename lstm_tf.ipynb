{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7MwbgzZCyTUA",
        "outputId": "88579177-59bd-44c1-dd54-4c0952965a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tsfresh==0.17.0\n",
            "  Downloading tsfresh-0.17.0-py2.py3-none-any.whl (91 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▋                            | 10 kB 19.4 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 20 kB 11.1 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 30 kB 8.5 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 40 kB 3.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 51 kB 4.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 61 kB 4.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 81 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 91 kB 3.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.25.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.3.5)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.1.1-py3-none-any.whl (830 kB)\n",
            "\u001b[K     |████████████████████████████████| 830 kB 40.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (0.5.2)\n",
            "Requirement already satisfied: statsmodels>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (0.10.2)\n",
            "Requirement already satisfied: tqdm>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (4.62.3)\n",
            "Requirement already satisfied: numpy>=1.15.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.19.5)\n",
            "Requirement already satisfied: requests>=2.9.1 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.2 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.0.2)\n",
            "Requirement already satisfied: dask[dataframe]>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (2.12.0)\n",
            "Requirement already satisfied: scipy>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from tsfresh==0.17.0) (1.4.1)\n",
            "Requirement already satisfied: toolz>=0.7.3 in /usr/local/lib/python3.7/dist-packages (from dask[dataframe]>=2.9.0->tsfresh==0.17.0) (0.11.2)\n",
            "Collecting partd>=0.3.10\n",
            "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting fsspec>=0.6.0\n",
            "  Downloading fsspec-2022.1.0-py3-none-any.whl (133 kB)\n",
            "\u001b[K     |████████████████████████████████| 133 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: sortedcontainers!=2.0.0,!=2.0.1 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (2.4.0)\n",
            "Requirement already satisfied: psutil>=5.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (5.4.8)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (2.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (21.3)\n",
            "Requirement already satisfied: zict>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (2.0.0)\n",
            "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (5.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (3.13)\n",
            "Requirement already satisfied: msgpack>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (1.0.3)\n",
            "Requirement already satisfied: click>=6.6 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (7.1.2)\n",
            "Requirement already satisfied: tblib>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (1.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from distributed>=2.11.0->tsfresh==0.17.0) (57.4.0)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2022.1.0-py3-none-any.whl (822 kB)\n",
            "\u001b[K     |████████████████████████████████| 822 kB 46.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.12.0-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 27.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.2-py3-none-any.whl (802 kB)\n",
            "\u001b[K     |████████████████████████████████| 802 kB 47.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.1-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 30.5 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.11.0-py3-none-any.whl (793 kB)\n",
            "\u001b[K     |████████████████████████████████| 793 kB 45.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.10.0-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 35.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.1-py3-none-any.whl (786 kB)\n",
            "\u001b[K     |████████████████████████████████| 786 kB 44.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.9.0-py3-none-any.whl (779 kB)\n",
            "\u001b[K     |████████████████████████████████| 779 kB 43.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.1-py3-none-any.whl (778 kB)\n",
            "\u001b[K     |████████████████████████████████| 778 kB 12.7 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.8.0-py3-none-any.whl (776 kB)\n",
            "\u001b[K     |████████████████████████████████| 776 kB 38.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.2-py3-none-any.whl (769 kB)\n",
            "\u001b[K     |████████████████████████████████| 769 kB 46.7 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.1-py3-none-any.whl (766 kB)\n",
            "\u001b[K     |████████████████████████████████| 766 kB 40.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.7.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 28.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.2-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 41.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.1-py3-none-any.whl (722 kB)\n",
            "\u001b[K     |████████████████████████████████| 722 kB 48.4 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.6.0-py3-none-any.whl (715 kB)\n",
            "\u001b[K     |████████████████████████████████| 715 kB 47.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.5.1-py3-none-any.whl (705 kB)\n",
            "\u001b[K     |████████████████████████████████| 705 kB 43.7 MB/s \n",
            "\u001b[?25hCollecting cloudpickle>=1.5.0\n",
            "  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n",
            "Collecting distributed>=2.11.0\n",
            "  Downloading distributed-2021.5.0-py3-none-any.whl (699 kB)\n",
            "\u001b[K     |████████████████████████████████| 699 kB 40.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.1-py3-none-any.whl (696 kB)\n",
            "\u001b[K     |████████████████████████████████| 696 kB 36.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.4.0-py3-none-any.whl (684 kB)\n",
            "\u001b[K     |████████████████████████████████| 684 kB 47.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.1-py3-none-any.whl (679 kB)\n",
            "\u001b[K     |████████████████████████████████| 679 kB 14.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.3.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 45.0 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.2.0-py3-none-any.whl (675 kB)\n",
            "\u001b[K     |████████████████████████████████| 675 kB 48.3 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.1-py3-none-any.whl (672 kB)\n",
            "\u001b[K     |████████████████████████████████| 672 kB 37.9 MB/s \n",
            "\u001b[?25h  Downloading distributed-2021.1.0-py3-none-any.whl (671 kB)\n",
            "\u001b[K     |████████████████████████████████| 671 kB 45.8 MB/s \n",
            "\u001b[?25h  Downloading distributed-2020.12.0-py3-none-any.whl (669 kB)\n",
            "\u001b[K     |████████████████████████████████| 669 kB 49.1 MB/s \n",
            "\u001b[?25h  Downloading distributed-2.30.1-py3-none-any.whl (656 kB)\n",
            "\u001b[K     |████████████████████████████████| 656 kB 45.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh==0.17.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.25.0->tsfresh==0.17.0) (2018.9)\n",
            "Collecting locket\n",
            "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from patsy>=0.4.1->tsfresh==0.17.0) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.1->tsfresh==0.17.0) (2.10)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.2->tsfresh==0.17.0) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.2->tsfresh==0.17.0) (3.0.0)\n",
            "Requirement already satisfied: heapdict in /usr/local/lib/python3.7/dist-packages (from zict>=0.1.3->distributed>=2.11.0->tsfresh==0.17.0) (1.0.1)\n",
            "Installing collected packages: locket, partd, fsspec, cloudpickle, distributed, tsfresh\n",
            "  Attempting uninstall: cloudpickle\n",
            "    Found existing installation: cloudpickle 1.3.0\n",
            "    Uninstalling cloudpickle-1.3.0:\n",
            "      Successfully uninstalled cloudpickle-1.3.0\n",
            "  Attempting uninstall: distributed\n",
            "    Found existing installation: distributed 1.25.3\n",
            "    Uninstalling distributed-1.25.3:\n",
            "      Successfully uninstalled distributed-1.25.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\u001b[0m\n",
            "Successfully installed cloudpickle-2.0.0 distributed-2.30.1 fsspec-2022.1.0 locket-0.2.1 partd-1.2.0 tsfresh-0.17.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "cloudpickle"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.19.5)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Collecting yfinance\n",
            "  Downloading yfinance-0.1.70-py2.py3-none-any.whl (26 kB)\n",
            "Collecting lxml>=4.5.1\n",
            "  Downloading lxml-4.7.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (6.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 3.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from yfinance) (1.19.5)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from yfinance) (0.0.10)\n",
            "Collecting requests>=2.26\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->yfinance) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=0.24.0->yfinance) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (2.0.11)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.26->yfinance) (1.24.3)\n",
            "Installing collected packages: requests, lxml, yfinance\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "  Attempting uninstall: lxml\n",
            "    Found existing installation: lxml 4.2.6\n",
            "    Uninstalling lxml-4.2.6:\n",
            "      Successfully uninstalled lxml-4.2.6\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed lxml-4.7.1 requests-2.27.1 yfinance-0.1.70\n"
          ]
        }
      ],
      "source": [
        "!pip install tsfresh==0.17.0\n",
        "!pip install h5py\n",
        "!pip install yfinance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBYULQP68Tl-",
        "outputId": "c6bc3c38-2f8f-4479-e7a8-011b152042a1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import datetime\n",
        "from random import sample\n",
        "import statistics\n",
        "from keras.models import model_from_json\n",
        "import xgboost as xgb\n",
        "from sklearn.datasets import make_multilabel_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "from tsfresh.feature_extraction import extract_features\n",
        "from tsfresh.feature_selection import select_features\n",
        "from tsfresh.feature_extraction import ComprehensiveFCParameters\n",
        "from tsfresh.feature_extraction import MinimalFCParameters\n",
        "\n",
        "import pickle\n",
        "import joblib as jb\n",
        "import IPython\n",
        "import IPython.display\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import DataFrame\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "import tsfresh as tfrs\n",
        "import json\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import random\n",
        "\n",
        "mpl.rcParams['figure.figsize'] = (8, 6)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "#Parametres\n",
        "pred_len = 5\n",
        "out_step = 30\n",
        "MAX_EPOCHS = 500\n",
        "history_lgt = 1000\n",
        "patience_v = 25\n",
        "\n",
        "#model metric\n",
        "loss_metric = tf.losses.MeanSquaredError()\n",
        "obj_metric = tf.metrics.MeanAbsoluteError()\n",
        "#baseline metric\n",
        "b_loss_metric = tf.losses.MeanSquaredError()\n",
        "b_obj_metric = tf.metrics.MeanAbsoluteError()\n",
        "\n",
        "#train_p > test_p leave some space for validation\n",
        "train_p = 0.60\n",
        "test_p = 0.30\n",
        "\n",
        "#lstm params\n",
        "n_layers = 5\n",
        "n_units = 1\n",
        "n_models_t = 8\n",
        "\n",
        "# configure bootstrap\n",
        "n_iterations = 10\n",
        "boot_percent = 0.90\n",
        "\n",
        "#configure metalearner\n",
        "tolerance = 0.01\n",
        "\n",
        "#XGBoost params\n",
        "test_s = 0.10\n",
        "xgb_estimator = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
        "                  colsample_bynode=1, colsample_bytree=1, gamma=0,\n",
        "                  learning_rate=0.1, max_delta_step=0, max_depth=15, n_iterations = 1000,\n",
        "                  min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
        "                  nthread=None, objective='binary:logistic', eval_metric = 'error', random_state=0,\n",
        "                  reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
        "                  silent=None, subsample=1, verbosity=1)\n",
        "\n",
        "multi_val_performance = {}\n",
        "val_performance = {}\n",
        "multi_performance = {}\n",
        "performance = {}\n",
        "lstm_models = {}\n",
        "models = {}\n",
        "training_mat = {}\n",
        "xgb_models = {}\n",
        "sel_mat = {}\n",
        "\n",
        "with open(\"/content/drive/MyDrive/Models/securities_dict_01_2022.json\") as json_file:\n",
        "  tsx_securities_dict = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AYe58ZkFc8TS"
      },
      "outputs": [],
      "source": [
        "def get_current_price(symbol):\n",
        "    ticker = yf.Ticker(symbol+'.TO')\n",
        "    todays_data = ticker.history(period='1d')\n",
        "    return todays_data['Close'][0]\n",
        "\n",
        "def fetch_finance_data(hist_lgt, ticker_list):\n",
        "  df_empty = pd.DataFrame()\n",
        "\n",
        "  for keys in list(ticker_list.keys()):\n",
        "    keys = keys.replace(\".\", \"-\")\n",
        "    money_df = pd.DataFrame(yf.download(f'{keys}.TO',\n",
        "                          progress=False))\n",
        "    #duplicate index problems\n",
        "    money_df = money_df[~money_df.index.duplicated(keep='first')]\n",
        "    if(len(money_df[\"Close\"])>=hist_lgt):\n",
        "      df_empty[keys] = money_df[\"Close\"].iloc[-hist_lgt:]\n",
        "  df_empty['Date'] = df_empty.index\n",
        "  return df_empty.dropna()\n",
        "\n",
        "def partition (list_in, n):\n",
        "    random.shuffle(list(list_in))\n",
        "    return [list_in[i::n] for i in range(n)]\n",
        "\n",
        "def assemble_matrix(training_mat):\n",
        "    for j in range(0,len(training_mat)):\n",
        "      if j==0:\n",
        "        table_ass = training_mat[j].iloc[:,:-1]\n",
        "      if j>0:\n",
        "        table_ass = pd.concat([table_ass, training_mat[j].iloc[:,:-1]], axis=1, join='inner')\n",
        "    for j in range(0,len(training_mat)):\n",
        "        table_ass[training_mat[j].columns[-1]]= training_mat[j].iloc[:,-1]\n",
        "    return table_ass\n",
        "\n",
        "def train_xgb(num_label, training_m, test_size, xgb_estimator):\n",
        "    X = training_m.iloc[:-1,:-num_label]\n",
        "    y = training_m.iloc[1:,-num_label:]\n",
        "    z = training_m.iloc[:,:-num_label]\n",
        "    # split dataset into training and test set\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=123, shuffle=False)\n",
        "    # create XGBoost instance with default hyper-parameters\n",
        "\n",
        "    # create MultiOutputClassifier instance with XGBoost model inside\n",
        "    multilabel_model = MultiOutputClassifier(xgb_estimator)\n",
        "    multilabel_model.fit(X_train, y_train)\n",
        "\n",
        "    z_predict = multilabel_model.predict(z)\n",
        "\n",
        "    return multilabel_model, z_predict, z\n",
        "\n",
        "def gen_label(bs_table, sel_mat):\n",
        "  df = sel_mat.iloc[-(bs_table.shape[0]+1):,-1].pct_change().dropna()\n",
        "  df.index = bs_table.index\n",
        "  df[df > 0] = 1 ; df[df <= 0] = 0\n",
        "  return df\n",
        "\n",
        "def reassemble_df_bs(models, loaded_df):\n",
        "  predict_df = models.predict(loaded_df[0].test)[:,:,:]\n",
        "  bite_lgt = predict_df.shape[1]\n",
        "  num_bite = predict_df.shape[0]\n",
        "  df = pd.DataFrame(np.concatenate(predict_df[:,:,:][0][0:(bite_lgt)]))\n",
        "  for j in range(1,num_bite):\n",
        "    df.loc[bite_lgt+j] = predict_df[:,:,:][j][(bite_lgt-1)]  # adding a row\n",
        "  return df\n",
        "\n",
        "def reassemble_full_bs(models, loaded_df):\n",
        "  for i in models:\n",
        "    if(i == 0):\n",
        "      df = reassemble_df_bs(models[i], loaded_df)\n",
        "    if(i > 0):\n",
        "      df[str(i)] = reassemble_df_bs(models[i], loaded_df)\n",
        "  return df\n",
        "\n",
        "def build_bs_ts(df, loaded_df):\n",
        "  df_lgt = df.shape[0] ; bs_lgt = df.shape[1]\n",
        "  res = pd.DataFrame(df.iloc[0,:]) ; res.index = range(0,bs_lgt)\n",
        "  for i in range(1,df_lgt):\n",
        "    for j in range(0,bs_lgt):\n",
        "      res.loc[i*bs_lgt + j] = df.iloc[i,j]\n",
        "  res.columns = [loaded_df[1].columns[-1]]\n",
        "  return res\n",
        "\n",
        "def error_table(bs_t, loaded_df):\n",
        "  rl_value = loaded_df[1].iloc[1:,-1]\n",
        "  rl_value.index = bs_t.index\n",
        "  for j in range(0,bs_t.shape[1]):\n",
        "    bs_t.iloc[:,j] = abs(bs_t.iloc[:,j] - rl_value)\n",
        "  return bs_t\n",
        "\n",
        "def groupedAvg(myArray, N=2):\n",
        "  result = myArray.groupby(myArray.index // N).sum()/N\n",
        "  return result\n",
        "\n",
        "def fetch_extract_features_df(ts_data, label_col_name, pred_len, custom_params = None, cl_label = None):\n",
        "    rm = ts_data[label_col_name].size%pred_len\n",
        "    len_df = ts_data[label_col_name].size\n",
        "    if(rm!= 0):\n",
        "      ts_data = ts_data[rm:len_df]\n",
        "\n",
        "    df  = DataFrame(ts_data[label_col_name])\n",
        "    df2 = DataFrame(np.repeat(range(int(len_df/pred_len)),pred_len))\n",
        "    df3 = DataFrame(np.tile(range(pred_len),int(len_df/pred_len)))\n",
        "    df2.index = df3.index = df.index\n",
        "    df[\"ID\"] = df2\n",
        "    df[\"Time\"] = df3\n",
        "    if(custom_params is None):\n",
        "      df_target = df[label_col_name][(pred_len-1)::pred_len]\n",
        "\n",
        "    #feature computing and selecting\n",
        "    if(custom_params is None):\n",
        "      settings = ComprehensiveFCParameters()\n",
        "    if(custom_params is not None):\n",
        "      settings = custom_params\n",
        "      #calculating predicted return change and rearranging index\n",
        "      df_target = groupedAvg(df[label_col_name],N=pred_len)[1:]\n",
        "      df_target2 = groupedAvg(df[label_col_name],N=pred_len)[:-1]\n",
        "      df_target.index = df_target2.index; df_target3 = df_target - df_target2\n",
        "      df_target3.loc[-1] = 0  # adding a row\n",
        "      df_target3.index = df_target3.index + 1  # shifting index\n",
        "      df_target3 = df_target3.sort_index()  # sorting by index\n",
        "      df_target = df_target3\n",
        "\n",
        "    extracted_features = extract_features(df, default_fc_parameters=settings, column_id=\"ID\", column_sort=\"Time\")\n",
        "\n",
        "\n",
        "    df_target.index = extracted_features.index\n",
        "    extracted_features = pd.DataFrame.dropna(extracted_features, axis = 1)\n",
        "    if(custom_params is None):\n",
        "      selected_features = select_features(extracted_features, df_target)\n",
        "      #adding price for lstm target value\n",
        "      selected_features[label_col_name] = df_target\n",
        "    if(custom_params is not None):\n",
        "      selected_features = extracted_features\n",
        "      selected_features[label_col_name] = df_target\n",
        "\n",
        "    return selected_features\n",
        "\n",
        "def train_lstm(selected_features,pred_len,out_step,MAX_EPOCHS,patience_v,loss_metric,obj_metric,\n",
        "               b_loss_metric,b_obj_metric,train_p,test_p,n_layers,n_units, skip_training):\n",
        "    #data normalisation\n",
        "    label_col_name = selected_features.columns[-1]\n",
        "    column_indices = {name: i for i, name in enumerate(selected_features.columns)}\n",
        "    n = len(selected_features)\n",
        "    train_df = selected_features[0:int(n*train_p)]\n",
        "    val_df = selected_features[int(n*train_p):int(n*(1-test_p))]\n",
        "    test_df = selected_features[int(n*(1-test_p)):]\n",
        "    predict_df = selected_features[-(out_step+1):]\n",
        "    predict_df.append([predict_df.iloc[-1:,:]],ignore_index=True)\n",
        "\n",
        "    num_features = selected_features.shape[1]\n",
        "\n",
        "    train_mean = train_df.mean()\n",
        "    train_std = train_df.std()\n",
        "\n",
        "    train_df = (train_df - train_mean) / train_std\n",
        "    val_df = (val_df - train_mean) / train_std\n",
        "    test_df = (test_df - train_mean) / train_std\n",
        "    predict_df = (predict_df - train_mean) / train_std\n",
        "\n",
        "    #TensorFlow window generator\n",
        "    class WindowGenerator():\n",
        "      def __init__(self, input_width, label_width, shift,\n",
        "                  train_df=train_df, val_df=val_df, test_df=test_df, predict_df=predict_df,\n",
        "                  label_columns=None):\n",
        "        # Store the raw data.\n",
        "        self.train_df = train_df\n",
        "        self.val_df = val_df\n",
        "        self.test_df = test_df\n",
        "        self.predict_df = predict_df\n",
        "\n",
        "        # Work out the label column indices.\n",
        "        self.label_columns = label_columns\n",
        "        if label_columns is not None:\n",
        "          self.label_columns_indices = {name: i for i, name in\n",
        "                                        enumerate(label_columns)}\n",
        "        self.column_indices = {name: i for i, name in\n",
        "                              enumerate(train_df.columns)}\n",
        "\n",
        "        # Work out the window parameters.\n",
        "        self.input_width = input_width\n",
        "        self.label_width = label_width\n",
        "        self.shift = shift\n",
        "\n",
        "        self.total_window_size = input_width + shift\n",
        "\n",
        "        self.input_slice = slice(0, input_width)\n",
        "        self.input_indices = np.arange(self.total_window_size)[self.input_slice]\n",
        "        self.predict_slice = slice(0, self.total_window_size)\n",
        "        self.predict_indices = np.arange(self.total_window_size)[self.predict_slice]\n",
        "\n",
        "        self.label_start = self.total_window_size - self.label_width\n",
        "        self.labels_slice = slice(self.label_start, None)\n",
        "        self.label_indices = np.arange(self.total_window_size)[self.labels_slice]\n",
        "\n",
        "      def __repr__(self):\n",
        "        return '\\n'.join([\n",
        "            f'Total window size: {self.total_window_size}',\n",
        "            f'Input indices: {self.input_indices}',\n",
        "            f'Label indices: {self.label_indices}',\n",
        "            f'Label column name(s): {self.label_columns}'])\n",
        "\n",
        "    def split_window(self, features):\n",
        "      inputs = features[:, self.input_slice, :]\n",
        "      labels = features[:, self.labels_slice, :]\n",
        "      if self.label_columns is not None:\n",
        "        labels = tf.stack(\n",
        "            [labels[:, :, self.column_indices[name]] for name in self.label_columns],\n",
        "            axis=-1)\n",
        "      # Slicing doesn't preserve static shape information, so set the shapes\n",
        "      # manually. This way the `tf.data.Datasets` are easier to inspect.\n",
        "      inputs.set_shape([None, self.input_width, None])\n",
        "      labels.set_shape([None, self.label_width, None])\n",
        "\n",
        "      return inputs, labels\n",
        "\n",
        "\n",
        "    WindowGenerator.split_window = split_window\n",
        "\n",
        "    def plot(self, model=None, plot_col=label_col_name, max_subplots=3):\n",
        "      inputs, labels = self.example\n",
        "      plt.figure(figsize=(12, 8))\n",
        "      plot_col_index = self.column_indices[plot_col]\n",
        "      max_n = min(max_subplots, len(inputs))\n",
        "      for n in range(max_n):\n",
        "        plt.subplot(3, 1, n+1)\n",
        "        plt.ylabel(f'{plot_col} [normed]')\n",
        "        plt.plot(self.input_indices, inputs[n, :, plot_col_index],\n",
        "                label='Inputs', marker='.', zorder=-10)\n",
        "\n",
        "        if self.label_columns:\n",
        "          label_col_index = self.label_columns_indices.get(plot_col, None)\n",
        "        else:\n",
        "          label_col_index = plot_col_index\n",
        "\n",
        "        if label_col_index is None:\n",
        "          continue\n",
        "\n",
        "        plt.scatter(self.label_indices, labels[n, :, label_col_index],\n",
        "                    edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
        "        if model is not None:\n",
        "          predictions = model(inputs)\n",
        "          plt.scatter(self.label_indices, predictions[n, :, label_col_index],\n",
        "                      marker='X', edgecolors='k', label='Predictions',\n",
        "                      c='#ff7f0e', s=64)\n",
        "\n",
        "        if n == 0:\n",
        "          plt.legend()\n",
        "\n",
        "      plt.xlabel('Time [h]')\n",
        "\n",
        "    WindowGenerator.plot = plot\n",
        "\n",
        "    def make_dataset(self, data):\n",
        "      data = np.array(data, dtype=np.float32)\n",
        "      ds = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
        "          data=data,\n",
        "          targets=None,\n",
        "          sequence_length=self.total_window_size,\n",
        "          sequence_stride=1,\n",
        "          shuffle=False,\n",
        "          batch_size=32,)\n",
        "\n",
        "      ds = ds.map(self.split_window)\n",
        "\n",
        "      return ds\n",
        "\n",
        "    WindowGenerator.make_dataset = make_dataset\n",
        "\n",
        "    @property\n",
        "    def train(self):\n",
        "      return self.make_dataset(self.train_df)\n",
        "    @property\n",
        "    def val(self):\n",
        "      return self.make_dataset(self.val_df)\n",
        "    @property\n",
        "    def test(self):\n",
        "      return self.make_dataset(self.test_df)\n",
        "    @property\n",
        "    def predict(self):\n",
        "      return self.make_dataset(self.predict_df)\n",
        "    @property\n",
        "    def example(self):\n",
        "      \"\"\"Get and cache an example batch of `inputs, labels` for plotting.\"\"\"\n",
        "      result = getattr(self, '_example', None)\n",
        "      if result is None:\n",
        "        # No example batch was found, so get one from the `.train` dataset\n",
        "        result = next(iter(self.train))\n",
        "        # And cache it for next time\n",
        "        self._example = result\n",
        "      return result\n",
        "\n",
        "    WindowGenerator.train = train\n",
        "    WindowGenerator.val = val\n",
        "    WindowGenerator.test = test\n",
        "    WindowGenerator.predict = predict\n",
        "    WindowGenerator.example = example\n",
        "\n",
        "    def compile_and_fit(model, window, patience=2):\n",
        "      early_stopping = tf.keras.callbacks.EarlyStopping(monitor='loss',\n",
        "                                                        patience=patience_v,\n",
        "                                                        mode='min')\n",
        "\n",
        "      model.compile(loss=loss_metric,\n",
        "                    optimizer=tf.optimizers.Adam(),\n",
        "                    metrics=[obj_metric])\n",
        "\n",
        "      history = model.fit(window.train, epochs=MAX_EPOCHS,\n",
        "                          validation_data=window.val,\n",
        "                          callbacks=[early_stopping])\n",
        "      return history\n",
        "\n",
        "    class Baseline(tf.keras.Model):\n",
        "      def __init__(self, label_index=None):\n",
        "        super().__init__()\n",
        "        self.label_index = label_index\n",
        "\n",
        "      def call(self, inputs):\n",
        "        if self.label_index is None:\n",
        "          return inputs\n",
        "        result = inputs[:, :, self.label_index]\n",
        "        return result[:, :, tf.newaxis]\n",
        "\n",
        "    baseline = Baseline(label_index=column_indices[label_col_name])\n",
        "\n",
        "    baseline.compile(loss=b_loss_metric,\n",
        "                    metrics=[b_obj_metric])\n",
        "\n",
        "    #Window generation\n",
        "    wide_window = WindowGenerator(\n",
        "        input_width=out_step, label_width=out_step, shift=1,\n",
        "        label_columns=[label_col_name])\n",
        "    if skip_training == False:\n",
        "      #lstm training and output\n",
        "      lstm_model = tf.keras.models.Sequential([\n",
        "          # Shape [batch, time, features] => [batch, time, lstm_units]\n",
        "          tf.keras.layers.LSTM(n_layers, return_sequences=True),\n",
        "          # Shape => [batch, time, features]\n",
        "          tf.keras.layers.Dense(units=n_units)\n",
        "      ])\n",
        "      history = compile_and_fit(lstm_model, wide_window)\n",
        "      return lstm_model, wide_window, test_df, predict_df\n",
        "    if skip_training == True:\n",
        "      return wide_window, test_df, predict_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf6I_og3xouJ"
      },
      "source": [
        "**Training LSTM**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "nxyZno2nojpz"
      },
      "outputs": [],
      "source": [
        "for j in range(3,4):\n",
        "  #only selecting our desired history length\n",
        "  ts_data = fetch_finance_data(history_lgt,tsx_securities_dict)\n",
        "  label_col_name = ts_data.columns.values[j]\n",
        "  #from the time series we extract relevant features using tsfresh, outputs lstm training matrix\n",
        "  selected_features = fetch_extract_features_df(ts_data, label_col_name, pred_len)\n",
        "  #saving training matrix with pickle\n",
        "  with open(\"/content/drive/MyDrive/Models_df/\"+str(label_col_name)+\".pickle\", 'wb') as handle:\n",
        "    pickle.dump(selected_features, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "  #fetching the numbers of column to slect for bootstrap\n",
        "  n_size = int((len(selected_features.columns)-1)*boot_percent)\n",
        "  lstm_models = {}\n",
        "  for i in range(0,n_iterations):\n",
        "    #randomly selecting a percentage of the columns (features) for bootstrapping procedure\n",
        "    boot = sample(range(0,(len(selected_features.columns)-2)), n_size)\n",
        "    #storing bootstrap table in variable\n",
        "    selected_features2 = selected_features.iloc[:,boot]\n",
        "    selected_features2[label_col_name] = selected_features[label_col_name]\n",
        "    #training lstm model and fetching window object and partial training matrix for testing and predicting\n",
        "    lstm_model, wide_window, test_df, predict_df = train_lstm(selected_features,pred_len,out_step,MAX_EPOCHS,patience_v,loss_metric,obj_metric,\n",
        "                  b_loss_metric,b_obj_metric,train_p,test_p,n_layers,n_units, skip_training = False)\n",
        "    model_json = lstm_model.to_json()\n",
        "    # serialize model to JSON\n",
        "    with open(\"/content/drive/MyDrive/Models/\"+str(label_col_name)+str(i)+\".json\", \"w\") as json_file:\n",
        "        json_file.write(model_json)\n",
        "    # serialize weights to HDF5\n",
        "    lstm_model.save_weights(\"/content/drive/MyDrive/Models/\"+str(label_col_name)+str(i)+\".h5\")\n",
        "    print(\"Saved model to disk\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZYoh7N2xu8X"
      },
      "source": [
        "**Loading models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Qb4ycluRZyVI",
        "outputId": "fcdffb4b-d9af-4d25-dda6-bc9fc51b9568"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "Loaded model from disk\n",
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f062eb2b5f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f062e391680> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Feature Extraction: 100%|██████████| 5/5 [00:00<00:00, 122.22it/s]\n",
            "Feature Extraction: 100%|██████████| 5/5 [00:00<00:00, 125.37it/s]\n",
            "Feature Extraction: 100%|██████████| 5/5 [00:00<00:00, 130.70it/s]\n",
            "Feature Extraction: 100%|██████████| 5/5 [00:00<00:00, 116.35it/s]\n"
          ]
        }
      ],
      "source": [
        "loaded_models = {}; loaded_df = {}\n",
        "for j in range(0,4):\n",
        "  ts_data = fetch_finance_data(history_lgt,tsx_securities_dict)\n",
        "  label_col_name = ts_data.columns.values[j]\n",
        "  loaded_model = {}\n",
        "  for i in range(0,n_iterations):\n",
        "    # load json and create model\n",
        "    json_file = open(\"/content/drive/MyDrive/Models/\"+str(label_col_name)+str(i)+\".json\", 'r')\n",
        "    loaded_model_json = json_file.read()\n",
        "    json_file.close()\n",
        "    loaded_model[i] = model_from_json(loaded_model_json)\n",
        "    # load weights into new model\n",
        "    loaded_model[i].load_weights(\"/content/drive/MyDrive/Models/\"+str(label_col_name)+str(i)+\".h5\")\n",
        "    # recompile model using precalculated weights\n",
        "    loaded_model[i].compile(loss=loss_metric, optimizer=tf.optimizers.Adam(), metrics=[obj_metric])\n",
        "    print(\"Loaded model from disk\")\n",
        "  loaded_models[j] = loaded_model\n",
        "  #fetching lstm training matrix\n",
        "  with open(\"/content/drive/MyDrive/Models_df/\"+str(label_col_name)+\".pickle\", 'rb') as handle:\n",
        "    selected_feature = pickle.load(handle)\n",
        "  #only fetching window object and partial training matrix, skipping lstm training\n",
        "  loaded_df[j] = train_lstm(selected_feature,pred_len,out_step,MAX_EPOCHS,patience_v,loss_metric,obj_metric,\n",
        "                b_loss_metric,b_obj_metric,train_p,test_p,n_layers,n_units, skip_training = True)\n",
        "  sel_mat[j] = selected_feature\n",
        "\n",
        "for i in range(0,4):\n",
        "  # assembling every test prediction of the bootstrap into one fat dataframe\n",
        "  bs_table = reassemble_full_bs(loaded_models[i], loaded_df[i])\n",
        "  # calculating real errors from this fat dataframe\n",
        "  er_tabble = error_table(bs_table,loaded_df[i])\n",
        "  # putting this dataframe as one big time series by transposing every row and contacatenating them all\n",
        "  bs_ts = build_bs_ts(er_tabble, loaded_df[i])\n",
        "  # using this big timeseries to extract features from the variation of errors from the bootstrap at every timestep (xgboost training matrix)\n",
        "  bs_features = fetch_extract_features_df(bs_ts, loaded_df[i][1].columns[-1], n_iterations, custom_params=MinimalFCParameters())\n",
        "  # generating -1 and 1 label if the stock goes up or down\n",
        "  bs_features[loaded_df[i][1].columns[-1]+\"_label\"] = gen_label(bs_features, sel_mat[i])\n",
        "  # storing the training matrix\n",
        "  training_mat[i] = bs_features\n",
        "  with open(\"/content/drive/MyDrive/Models_df/bs_ts\"+str(i)+\".pickle\", 'wb') as handle:\n",
        "    pickle.dump(training_mat[i], handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "KgWYQM1NC4Gl"
      },
      "outputs": [],
      "source": [
        "training_mat = {}\n",
        "with open(\"/content/drive/MyDrive/Models/securities_dict_01_2022.json\") as json_file:\n",
        "  tsx_securities_dict = json.load(json_file)\n",
        "for j in range(0,4):\n",
        "  with open(\"/content/drive/MyDrive/Models_df/bs_ts\"+str(j)+\".pickle\", 'rb') as handle:\n",
        "    training_mat[j] = pickle.load(handle)\n",
        "\n",
        "ass_mat = assemble_matrix(training_mat)\n",
        "xgb_model = train_xgb(4, ass_mat.iloc[1:,:], 0.2, xgb_estimator)\n",
        "next_prediction = xgb_model[1][-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Oax9m_Q0TSik"
      },
      "outputs": [],
      "source": [
        "def return_last_1(n):\n",
        "  return n[-1][1]\n",
        "\n",
        "x = map(return_last_1, xgb_model[0].predict_proba(xgb_model[2]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Czn4ZTouV26M",
        "outputId": "0328f776-0314-438c-8a59-a552950cbed9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['AC', 'SHOP', 'SU', 'TD']\n",
            "[0.13389073 0.34020227 0.30489552 0.22101142]\n"
          ]
        }
      ],
      "source": [
        "\n",
        "result_array = np.array(list(x))\n",
        "print(list(tsx_securities_dict.keys())[0:4])\n",
        "print(result_array/sum(result_array))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "dqQ4xQzaZYCx",
        "outputId": "45fa35e6-4a0d-4d1c-a48f-0d39a7b51cd0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.02972176 0.03009943 0.         0.         0.04984279 0.\n",
            " 0.03352922 0.03613449 0.02772861 0.02753341 0.04013626 0.\n",
            " 0.         0.0540335  0.         0.03379419 0.01681332 0.03150366\n",
            " 0.04594743 0.07191212 0.         0.         0.08945449 0.\n",
            " 0.05454541 0.03914146 0.05595576 0.03742428 0.02333892 0.\n",
            " 0.         0.03555537 0.         0.03651338 0.02712697 0.07221375]\n",
            "Index(['AC__sum_values', 'AC__median', 'AC__mean', 'AC__length',\n",
            "       'AC__standard_deviation', 'AC__variance', 'AC__maximum', 'AC__minimum',\n",
            "       'AC', 'SHOP__sum_values', 'SHOP__median', 'SHOP__mean', 'SHOP__length',\n",
            "       'SHOP__standard_deviation', 'SHOP__variance', 'SHOP__maximum',\n",
            "       'SHOP__minimum', 'SHOP', 'SU__sum_values', 'SU__median', 'SU__mean',\n",
            "       'SU__length', 'SU__standard_deviation', 'SU__variance', 'SU__maximum',\n",
            "       'SU__minimum', 'SU', 'TD__sum_values', 'TD__median', 'TD__mean',\n",
            "       'TD__length', 'TD__standard_deviation', 'TD__variance', 'TD__maximum',\n",
            "       'TD__minimum', 'TD'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "feat_impts = []\n",
        "for clf in xgb_model[0].estimators_:\n",
        "    feat_impts.append(clf.feature_importances_)\n",
        "\n",
        "print(np.mean(feat_impts, axis=0))\n",
        "print(ass_mat.columns[:-4])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dND8Gr5SlSmX"
      },
      "outputs": [],
      "source": [
        "#Deprecated functions\n",
        "\n",
        "#ts_data, ts_metadata = pull_daily_time_series_alpha_vantage(alpha_vantage_api_key, ticker_name = ticker_n, output_size = \"full\")\n",
        "from alpha_vantage.timeseries import TimeSeries\n",
        "\n",
        "alpha_vantage_api_key = \"HTNGJULEJU3C38H5\"\n",
        "\n",
        "def pull_daily_time_series_alpha_vantage(alpha_vantage_api_key, ticker_name, output_size = \"compact\"):\n",
        "    \"\"\"\n",
        "    Pull daily time series by stock ticker name.\n",
        "    Args:\n",
        "        alpha_vantage_api_key: Str. Alpha Vantage API key.\n",
        "        ticker_name: Str. Ticker name that we want to pull.\n",
        "        output_size: Str. Can be \"full\" or \"compact\". If \"compact\", then the past 100 days of data\n",
        "        is returned. If \"full\" the complete time series is returned (could be 20 years' worth of data!)\n",
        "    Outputs:\n",
        "        data: Dataframe. Time series data, including open, high, low, close, and datetime values.\n",
        "        metadata: Dataframe. Metadata associated with the time series.\n",
        "    \"\"\"\n",
        "    #Generate Alpha Vantage time series object\n",
        "    ts = TimeSeries(key = alpha_vantage_api_key, output_format = 'pandas')\n",
        "    data, meta_data = ts.get_daily_adjusted(ticker_name, outputsize = output_size)\n",
        "    data['date_time'] = data.index\n",
        "    data = data[::-1]\n",
        "    return data, meta_data"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}